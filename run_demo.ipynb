{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ§  Atlas â€” Interactive Demo\n",
                "\n",
                "Welcome! This notebook walks you through the Atlas long-form memory system step by step.\n",
                "\n",
                "**What you'll learn:**\n",
                "1. How the agent extracts memories from conversation\n",
                "2. How memories are stored and retrieved\n",
                "3. How to test memory recall\n",
                "4. How to use different providers (Ollama, Gemini, Groq)\n",
                "\n",
                "> ğŸ“Š For full benchmarks and evaluation results, see **[results.ipynb](results.ipynb)**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup\n",
                "\n",
                "First, install dependencies and load the environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "# Add project root to path\n",
                "project_root = Path.cwd()\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "from src.agent import LongMemAgent\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "print('âœ“ Dependencies loaded')\n",
                "print(f'  Project root: {project_root}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Choose Your Provider\n",
                "\n",
                "Atlas supports multiple LLM backends. Pick one:\n",
                "\n",
                "| Provider | Model | API Key Needed | Notes |\n",
                "|----------|-------|:--------------:|-------|\n",
                "| `ollama` | `mistral` | No | Local, fast, free. Run `ollama serve` first |\n",
                "| `gemini` | `gemma-3-27b-it` | `GEMINI_API_KEY` | Free tier, 30 req/min |\n",
                "| `groq` | `llama-3.1-8b-instant` | `GROQ_API_KEY` | Very fast, rate limited |\n",
                "\n",
                "Edit the cell below to change providers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ”§ CONFIGURE YOUR PROVIDER HERE\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "PROVIDER = 'ollama'                 # 'ollama', 'gemini', or 'groq'\n",
                "MODEL    = 'mistral'                # model name for your provider\n",
                "# MODEL  = 'gemma-3-27b-it'         # uncomment for Gemini\n",
                "# MODEL  = 'llama-3.1-8b-instant'   # uncomment for Groq\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "# Clean up any previous demo database\n",
                "demo_db = 'demo_interactive.db'\n",
                "if os.path.exists(demo_db):\n",
                "    os.remove(demo_db)\n",
                "    print(f'ğŸ—‘ï¸  Cleaned previous {demo_db}')\n",
                "\n",
                "# Initialize agent\n",
                "agent = LongMemAgent(\n",
                "    provider=PROVIDER,\n",
                "    model=MODEL,\n",
                "    db_path=demo_db,\n",
                "    context_limit=4096,\n",
                "    flush_threshold=0.70\n",
                ")\n",
                "\n",
                "print(f'\\nâœ“ Agent initialized')\n",
                "print(f'  Provider: {agent.provider}')\n",
                "print(f'  Model:    {agent.model}')\n",
                "print(f'  DB:       {demo_db}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Chat Helper\n",
                "\n",
                "A nice helper to display conversations with memory metadata."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import display, Markdown, HTML\n",
                "\n",
                "def chat(message: str, show_memories=True):\n",
                "    \"\"\"Send a message and display the response with memory info.\"\"\"\n",
                "    print(f'\\nğŸ‘¤ You: {message}')\n",
                "    \n",
                "    response = agent.chat(message)\n",
                "    \n",
                "    print(f'ğŸ¤– Atlas: {response[\"response\"]}')\n",
                "    print(f'\\n   â”Œâ”€ Turn {response[\"turn_id\"]} â”‚ '\n",
                "          f'Context: {response[\"context_utilization\"]} â”‚ '\n",
                "          f'Memories: {response[\"total_memories\"]} â”‚ '\n",
                "          f'Flush: {\"yes\" if response[\"flush_triggered\"] else \"no\"}')\n",
                "    \n",
                "    if show_memories and response['active_memories']:\n",
                "        print(f'   â”‚ ğŸ§  Retrieved {len(response[\"active_memories\"])} memories:')\n",
                "        for mem in response['active_memories']:\n",
                "            print(f'   â”‚   â€¢ {mem[\"content\"]} '\n",
                "                  f'(from turn {mem[\"origin_turn\"]}, conf: {mem[\"confidence\"]:.2f})')\n",
                "    print('   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€')\n",
                "    \n",
                "    return response\n",
                "\n",
                "print('âœ“ Chat helper ready. Use chat(\"your message\") below!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Plant Some Memories\n",
                "\n",
                "Let's tell the agent some facts about ourselves. These will be extracted as memories."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chat('Hi! My name is Jordan and I work as a data scientist in Seattle.')\n",
                "chat(\"I'm allergic to peanuts and I follow a vegetarian diet.\")\n",
                "chat('My favorite programming language is Python. I also use R for statistics.')\n",
                "chat('I have a daughter named Maya, she is 5 years old.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Trigger Memory Distillation\n",
                "\n",
                "The agent normally distills memories when context hits 70% capacity.\n",
                "We can also trigger it manually to extract memories right now."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('âš¡ Triggering memory distillation...\\n')\n",
                "\n",
                "result = agent.manual_distill()\n",
                "\n",
                "print(f'âœ“ {result[\"message\"]}')\n",
                "print(f'  Memories added: {result[\"memories_added\"]}')\n",
                "print(f'  Total memories: {result[\"total_memories\"]}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inspect Stored Memories\n",
                "\n",
                "Let's see what the distiller extracted."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "memories = agent.get_all_memories()\n",
                "\n",
                "if memories:\n",
                "    print(f'ğŸ“¦ Stored Memories ({len(memories)} total):\\n')\n",
                "    for m in memories:\n",
                "        print(f\"  [{m['type']:>12}] {m['key']}: {m['value']}\")\n",
                "        print(f\"               confidence: {m['confidence']:.2f}, from turn {m['source_turn']}\")\n",
                "else:\n",
                "    print('No memories stored yet. Run distillation first.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Test Memory Recall\n",
                "\n",
                "Now the key test â€” does the agent remember what we told it?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chat(\"What's my name?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chat('Suggest a dinner recipe for me.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chat('What should I get my kid for her birthday?')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chat('Tell me everything you remember about me.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Try It Yourself!\n",
                "\n",
                "Use the cell below to have your own conversation with the agent.\n",
                "Each message is processed through the full memory pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Type your message here:\n",
                "chat('What do I do for work?')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try another:\n",
                "chat('Your message here')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Database Peek\n",
                "\n",
                "Under the hood, memories are stored in SQLite with vector embeddings + full-text search indexes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sqlite3\n",
                "\n",
                "conn = sqlite3.connect(demo_db)\n",
                "cursor = conn.cursor()\n",
                "\n",
                "cursor.execute('SELECT COUNT(*) FROM memories WHERE is_active = 1')\n",
                "active = cursor.fetchone()[0]\n",
                "\n",
                "cursor.execute('SELECT COUNT(*) FROM turns')\n",
                "turns = cursor.fetchone()[0]\n",
                "\n",
                "cursor.execute('''\n",
                "    SELECT type, COUNT(*) as count \n",
                "    FROM memories WHERE is_active = 1 \n",
                "    GROUP BY type\n",
                "''')\n",
                "types = cursor.fetchall()\n",
                "\n",
                "print(f'ğŸ“‹ Database: {demo_db}')\n",
                "print(f'   Active memories: {active}')\n",
                "print(f'   Total turns: {turns}')\n",
                "print(f'   Memory types:')\n",
                "for type_name, count in types:\n",
                "    print(f'     {type_name}: {count}')\n",
                "\n",
                "conn.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "| What | How |\n",
                "|:-----|:----|\n",
                "| ğŸ“Š **See benchmarks** | Open [results.ipynb](results.ipynb) |\n",
                "| ğŸ’¬ **Interactive CLI** | `uv run python main.py` |\n",
                "| ğŸ§ª **Run evaluation** | `uv run python eval/evaluate.py --provider gemini` |\n",
                "| ğŸ  **Local eval** | `uv run python eval/evaluate.py --local --model mistral` |\n",
                "| ğŸ“– **Quick eval** | `uv run python eval/evaluate.py --quick --local` |\n",
                "\n",
                "### Architecture\n",
                "\n",
                "```\n",
                "User Message\n",
                "    â”‚\n",
                "    â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚    Context Manager      â”‚ â† checks if context needs flushing\n",
                "â”‚    (token counting)     â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "         â”‚ if >70% full\n",
                "         â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚    Memory Distiller     â”‚ â† LLM extracts memories from conversation\n",
                "â”‚    (Groq/Gemini/Ollama) â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "         â”‚\n",
                "         â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚    Memory Store         â”‚ â† SQLite + sqlite-vec + FTS5\n",
                "â”‚    (persist & search)   â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "         â”‚\n",
                "         â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚    Memory Retriever     â”‚ â† hybrid vector + keyword search\n",
                "â”‚    (RRF fusion)         â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "         â”‚\n",
                "         â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚    System Prompt        â”‚ â† memories injected into context\n",
                "â”‚    + LLM Response       â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cleanup (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to delete the demo database:\n",
                "# os.remove(demo_db)\n",
                "# print(f'âœ“ Removed {demo_db}')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 4,
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}