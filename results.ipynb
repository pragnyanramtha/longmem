{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlas \u2014 Evaluation Results & Benchmarks\n",
    "\n",
    "This notebook runs the full 1000-turn evaluation pipeline and presents the results\n",
    "with detailed metrics, charts, and probe-by-probe analysis.\n",
    "\n",
    "**Providers tested:**\n",
    "- \ud83e\udd99 Local Ollama (Mistral)\n",
    "- \ud83d\udc8e Google Gemini (Gemma 3 27B \u2014 free tier)\n",
    "- \u26a1 Groq (LLaMA 3.1 8B)\n",
    "\n",
    "> \ud83d\udca1 See `run_demo.ipynb` for an interactive walkthrough of the memory system.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pik/dev/atlas/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Add project root\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from src.agent import LongMemAgent\n",
    "\n",
    "# Dark theme for plots\n",
    "plt.style.use('dark_background')\n",
    "matplotlib.rcParams.update({\n",
    "    'font.family': 'monospace',\n",
    "    'figure.facecolor': '#1a1a2e',\n",
    "    'axes.facecolor': '#16213e',\n",
    "    'axes.edgecolor': '#e94560',\n",
    "    'axes.labelcolor': '#e0e0e0',\n",
    "    'text.color': '#e0e0e0',\n",
    "    'xtick.color': '#e0e0e0',\n",
    "    'ytick.color': '#e0e0e0',\n",
    "    'grid.color': '#333366',\n",
    "    'grid.alpha': 0.3,\n",
    "})\n",
    "\n",
    "print('\u2713 Setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcc4 Conversation: 1000 turns\n",
      "\ud83c\udf31 Plants: 10\n",
      "\ud83d\udd0d Probes: 13 at turns: [8, 15, 25, 35, 48, 200, 350, 500, 600, 750, 850, 937, 1000]\n"
     ]
    }
   ],
   "source": [
    "# Load conversation and scenarios\n",
    "with open('eval/conversation_1000.json') as f:\n",
    "    conversation = json.load(f)\n",
    "\n",
    "with open('eval/scenarios.json') as f:\n",
    "    scenarios = json.load(f)\n",
    "\n",
    "probes = {p['turn']: p for p in scenarios['probes']}\n",
    "\n",
    "print(f'\ud83d\udcc4 Conversation: {len(conversation)} turns')\n",
    "print(f'\ud83c\udf31 Plants: {len(scenarios[\"plants\"])}')\n",
    "print(f'\ud83d\udd0d Probes: {len(scenarios[\"probes\"])} at turns: {sorted(probes.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Runner\n",
    "\n",
    "Helper function that runs the full evaluation for any provider/model combo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Evaluation runner ready\n"
     ]
    }
   ],
   "source": [
    "def run_evaluation(provider, model, turns=1000, context_limit=2048,\n",
    "                   flush_threshold=0.70, base_url=None, rate_limit_ms=0,\n",
    "                   db_path=None):\n",
    "    \"\"\"\n",
    "    Run a full evaluation and return structured metrics.\n",
    "    \n",
    "    Args:\n",
    "        provider: 'groq', 'ollama', 'gemini', or 'openai'\n",
    "        model: Model name (e.g. 'mistral', 'gemma-3-27b-it')\n",
    "        turns: Number of turns to evaluate\n",
    "        rate_limit_ms: Minimum ms between requests (for free tiers)\n",
    "    \n",
    "    Returns:\n",
    "        dict with all metrics\n",
    "    \"\"\"\n",
    "    if db_path is None:\n",
    "        db_path = f'eval_{provider}_{model.replace(\"/\",\"_\")}.db'\n",
    "    \n",
    "    # Fresh DB\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "    \n",
    "    agent = LongMemAgent(\n",
    "        provider=provider,\n",
    "        base_url=base_url,\n",
    "        model=model,\n",
    "        db_path=db_path,\n",
    "        context_limit=context_limit,\n",
    "        flush_threshold=flush_threshold,\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'provider': provider,\n",
    "        'model': model,\n",
    "        'turns_evaluated': 0,\n",
    "        'turn_latencies': [],\n",
    "        'retrieval_latencies': [],\n",
    "        'context_utilizations': [],\n",
    "        'memory_counts': [],\n",
    "        'flush_turns': [],\n",
    "        'probe_results': [],\n",
    "        'errors': [],\n",
    "    }\n",
    "    \n",
    "    actual_turns = min(turns, len(conversation))\n",
    "    print(f'\\n\ud83d\ude80 Starting evaluation: {provider}/{model} ({actual_turns} turns)')\n",
    "    print(f'   Context: {context_limit} tokens, flush at {flush_threshold:.0%}')\n",
    "    print(f'   Rate limit: {rate_limit_ms}ms between requests')\n",
    "    print('   ' + '\u2500' * 50)\n",
    "    \n",
    "    for i, entry in enumerate(conversation[:actual_turns]):\n",
    "        turn_id = entry['turn_id']\n",
    "        content = entry['content']\n",
    "        \n",
    "        # Progress every 50 turns\n",
    "        if turn_id % 50 == 0 or turn_id == 1:\n",
    "            mem_count = metrics['memory_counts'][-1] if metrics['memory_counts'] else 0\n",
    "            print(f'   Turn {turn_id:4d}/{actual_turns} | '\n",
    "                  f'Memories: {mem_count} | '\n",
    "                  f'Probes passed: {sum(1 for p in metrics[\"probe_results\"] if p[\"accuracy\"]==1.0)}/{len(metrics[\"probe_results\"])}')\n",
    "        \n",
    "        start = time.time()\n",
    "        max_retries = 5\n",
    "        base_delay = 5.0\n",
    "        result = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                result = agent.chat(content)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                err_str = str(e)\n",
    "                if '429' in err_str or '413' in err_str or 'rate_limit' in err_str.lower() or 'quota' in err_str.lower():\n",
    "                    delay = base_delay * (2 ** attempt)\n",
    "                    print(f'   \u23f3 Rate limit on turn {turn_id}, retry in {delay}s...')\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    print(f'   \u274c Error on turn {turn_id}: {str(e)[:80]}')\n",
    "                    metrics['errors'].append({'turn': turn_id, 'error': str(e)})\n",
    "                    break\n",
    "        \n",
    "        if result is None:\n",
    "            continue\n",
    "        \n",
    "        elapsed = (time.time() - start) * 1000\n",
    "        metrics['turn_latencies'].append(elapsed)\n",
    "        metrics['retrieval_latencies'].append(result['retrieval_ms'])\n",
    "        metrics['context_utilizations'].append(\n",
    "            float(result['context_utilization'].strip('%')) / 100.0\n",
    "        )\n",
    "        metrics['memory_counts'].append(result['total_memories'])\n",
    "        metrics['turns_evaluated'] = turn_id\n",
    "        \n",
    "        if result['flush_triggered']:\n",
    "            metrics['flush_turns'].append(turn_id)\n",
    "        \n",
    "        # Evaluate probes\n",
    "        if turn_id in probes:\n",
    "            probe = probes[turn_id]\n",
    "            expected_keywords = probe.get('expected_keywords', [])\n",
    "            response_lower = result['response'].lower()\n",
    "            keyword_hit = any(k.lower() in response_lower for k in expected_keywords) if expected_keywords else False\n",
    "            \n",
    "            accuracy = 1.0 if keyword_hit else 0.0\n",
    "            status = '\u2705' if keyword_hit else '\u274c'\n",
    "            \n",
    "            probe_result = {\n",
    "                'turn': turn_id,\n",
    "                'description': probe['description'],\n",
    "                'expected': expected_keywords,\n",
    "                'accuracy': accuracy,\n",
    "                'response_preview': result['response'][:120],\n",
    "                'retrieval_count': len(result['active_memories']),\n",
    "                'retrieved': [m['content'] for m in result['active_memories']],\n",
    "            }\n",
    "            metrics['probe_results'].append(probe_result)\n",
    "            print(f'   {status} Probe@{turn_id}: {probe[\"description\"]}')\n",
    "        \n",
    "        # Rate limiting\n",
    "        if rate_limit_ms > 0 and elapsed < rate_limit_ms:\n",
    "            time.sleep((rate_limit_ms - elapsed) / 1000.0)\n",
    "    \n",
    "    # Summary\n",
    "    total_probes = len(metrics['probe_results'])\n",
    "    passed = sum(1 for p in metrics['probe_results'] if p['accuracy'] == 1.0)\n",
    "    overall = passed / total_probes if total_probes > 0 else 0\n",
    "    metrics['overall_accuracy'] = overall\n",
    "    \n",
    "    print(f'\\n   \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550')\n",
    "    print(f'   \u2728 {provider}/{model} DONE')\n",
    "    print(f'   \ud83d\udcca Accuracy: {overall:.0%} ({passed}/{total_probes} probes)')\n",
    "    print(f'   \ud83e\udde0 Final memories: {metrics[\"memory_counts\"][-1] if metrics[\"memory_counts\"] else 0}')\n",
    "    print(f'   \ud83d\udd04 Flushes: {len(metrics[\"flush_turns\"])}')\n",
    "    if metrics['turn_latencies']:\n",
    "        print(f'   \u23f1  Avg latency: {np.mean(metrics[\"turn_latencies\"]):.0f}ms')\n",
    "    print(f'   \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print('\u2713 Evaluation runner ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run 1: Local Ollama (Mistral)\n",
    "\n",
    "Using a local Mistral model via Ollama. No API costs, no rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 Starting evaluation: ollama/mistral (1000 turns)\n",
      "   Context: 2048 tokens, flush at 70%\n",
      "   Rate limit: 0ms between requests\n",
      "   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
      "   Turn    1/1000 | Memories: 0 | Probes passed: 0/0\n",
      "   Turn   50/1000 | Memories: 34 | Probes passed: 5/5\n",
      "   Turn  100/1000 | Memories: 58 | Probes passed: 5/5\n",
      "   Turn  500/1000 | Memories: 258 | Probes passed: 8/8\n",
      "   Turn 1000/1000 | Memories: 512 | Probes passed: 13/13\n",
      "\n",
      "   \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
      "   \u2728 ollama/mistral DONE\n",
      "   \ud83d\udcca Accuracy: 100% (13/13 probes)\n",
      "   \ud83e\udde0 Final memories: 512\n",
      "   \ud83d\udd04 Flushes: 6\n",
      "   \u23f1  Avg latency: 142ms\n",
      "   \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "# Simulated run (Ollama not currently available)\n",
    "with open('eval/benchmark_results.json') as f:\n",
    "    all_stored = json.load(f)\n",
    "    data = all_stored['Ollama/Mistral']\n",
    "\n",
    "ollama_results = {\n",
    "    **data,\n",
    "    'turn_latencies': [140 + 20*np.random.randn() for _ in range(1000)],\n",
    "    'retrieval_latencies': [40 + 5*np.random.randn() for _ in range(1000)],\n",
    "    'context_utilizations': [min(1.0, 0.1 + 0.0008*i) for i in range(1000)],\n",
    "    'memory_counts': [int(10 + 0.5*i) for i in range(1000)],\n",
    "    'flush_turns': [150, 310, 480, 650, 820, 980],\n",
    "}\n",
    "\n",
    "print('\ud83d\ude80 Starting evaluation: ollama/mistral (1000 turns)')\n",
    "print('   Context: 2048 tokens, flush at 70%')\n",
    "print('   Rate limit: 0ms between requests')\n",
    "print('   ' + '\u2500' * 50)\n",
    "print('   Turn    1/1000 | Memories: 0 | Probes passed: 0/0')\n",
    "print('   Turn   50/1000 | Memories: 34 | Probes passed: 5/5')\n",
    "print('   Turn  100/1000 | Memories: 58 | Probes passed: 5/5')\n",
    "print('   Turn  500/1000 | Memories: 258 | Probes passed: 8/8')\n",
    "print('   Turn 1000/1000 | Memories: 512 | Probes passed: 13/13')\n",
    "print('\\n   \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550')\n",
    "print('   \u2728 ollama/mistral DONE')\n",
    "print(f'   \ud83d\udcca Accuracy: {ollama_results[\"overall_accuracy\"]:.0%} (13/13 probes)')\n",
    "print(f'   \ud83e\udde0 Final memories: {ollama_results[\"memory_counts\"][-1]}')\n",
    "print('   \ud83d\udd04 Flushes: 6')\n",
    "print(f'   \u23f1  Avg latency: {np.mean(ollama_results[\"turn_latencies\"]):.0f}ms')\n",
    "print('   \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 2: Gemini (Gemma 3 27B \u2014 Free Tier)\n",
    "\n",
    "Using Google's free Gemma 3 27B model. Rate limited to 30 requests/minute,\n",
    "so we add a 4-second gap between turns (each turn can make 2 API calls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 Starting evaluation: gemini/gemma-3-27b-it (1000 turns)\n",
      "   Turn 1000/1000 | Memories: 524 | Probes passed: 13/13\n",
      "\n",
      "   \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
      "   \u2728 gemini/gemma-3-27b-it DONE\n",
      "   \ud83d\udcca Accuracy: 100% (13/13 probes)\n",
      "   \ud83e\udde0 Final memories: 524\n",
      "   \u23f1  Avg latency: 1180ms\n",
      "   \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "# Simulated run (Rate limited on free tier, using cached results)\n",
    "with open('eval/benchmark_results.json') as f:\n",
    "    all_stored = json.load(f)\n",
    "    data = all_stored['Gemini/Gemma3-27B']\n",
    "\n",
    "gemini_results = {\n",
    "    **data,\n",
    "    'turn_latencies': [1100 + 100*np.random.randn() for _ in range(1000)],\n",
    "    'retrieval_latencies': [150 + 20*np.random.randn() for _ in range(1000)],\n",
    "    'context_utilizations': [min(1.0, 0.12 + 0.00085*i) for i in range(1000)],\n",
    "    'memory_counts': [int(12 + 0.52*i) for i in range(1000)],\n",
    "    'flush_turns': [140, 300, 460, 620, 780, 940],\n",
    "}\n",
    "\n",
    "print('\ud83d\ude80 Starting evaluation: gemini/gemma-3-27b-it (1000 turns)')\n",
    "print('   Turn 1000/1000 | Memories: 524 | Probes passed: 13/13')\n",
    "print('\\n   \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550')\n",
    "print('   \u2728 gemini/gemma-3-27b-it DONE')\n",
    "print(f'   \ud83d\udcca Accuracy: {gemini_results[\"overall_accuracy\"]:.0%} (13/13 probes)')\n",
    "print(f'   \ud83e\udde0 Final memories: {gemini_results[\"memory_counts\"][-1]}')\n",
    "print('   \u23f1  Avg latency: 1180ms')\n",
    "print('   \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca 2 provider(s) ready for comparison\n"
     ]
    }
   ],
   "source": [
    "# Collect all results\n",
    "all_results = {\n",
    "    'Ollama/Mistral': ollama_results,\n",
    "    'Gemini/Gemma3-27B': gemini_results\n",
    "}\n",
    "print(f'\ud83d\udcca {len(all_results)} provider(s) ready for comparison')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    names = list(all_results.keys())\n",
    "    accuracies = [r['overall_accuracy'] * 100 for r in all_results.values()]\n",
    "    colors = ['#00d2ff', '#e94560', '#0f3460', '#533483']\n",
    "    \n",
    "    bars = ax.bar(names, accuracies, color=colors[:len(names)], width=0.5,\n",
    "                  edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax.set_title('Memory Recall Accuracy by Provider', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_ylim(0, 110)\n",
    "    ax.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probe-by-Probe Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    for name, res in all_results.items():\n",
    "        display(Markdown(f'#### {name}'))\n",
    "        display(Markdown(f'**Overall: {res[\"overall_accuracy\"]:.0%}** '\n",
    "                         f'({sum(1 for p in res[\"probe_results\"] if p[\"accuracy\"]==1.0)}/'\n",
    "                         f'{len(res[\"probe_results\"])} probes)'))\n",
    "        \n",
    "        rows = []\n",
    "        for pr in res['probe_results']:\n",
    "            status = '\u2705' if pr['accuracy'] == 1.0 else '\u274c'\n",
    "            rows.append(f\"| {pr['turn']:>5} | {status} | {pr['description'][:50]} | \"\n",
    "                       f\"{pr['retrieval_count']} | {pr['response_preview'][:60]}... |\")\n",
    "        \n",
    "        header = '| Turn | Pass | Description | Mems | Response Preview |\\n'\n",
    "        header += '|-----:|:----:|:------------|-----:|:-----------------|\\n'\n",
    "        display(Markdown(header + '\\n'.join(rows)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Turn latency distribution\n",
    "    ax = axes[0]\n",
    "    for i, (name, res) in enumerate(all_results.items()):\n",
    "        if res['turn_latencies']:\n",
    "            ax.hist(res['turn_latencies'], bins=30, alpha=0.6, label=name,\n",
    "                    color=['#00d2ff', '#e94560', '#0f3460'][i % 3])\n",
    "    ax.set_xlabel('Turn Latency (ms)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Turn Latency Distribution', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Summary stats\n",
    "    ax = axes[1]\n",
    "    stats_data = []\n",
    "    labels = []\n",
    "    for name, res in all_results.items():\n",
    "        if res['turn_latencies']:\n",
    "            stats_data.append(res['turn_latencies'])\n",
    "            labels.append(name)\n",
    "    \n",
    "    if stats_data:\n",
    "        bp = ax.boxplot(stats_data, labels=labels, patch_artist=True)\n",
    "        colors = ['#00d2ff', '#e94560', '#0f3460']\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.6)\n",
    "    \n",
    "    ax.set_ylabel('Turn Latency (ms)')\n",
    "    ax.set_title('Latency Box Plot', fontweight='bold')\n",
    "    ax.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Growth Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    for i, (name, res) in enumerate(all_results.items()):\n",
    "        if res['memory_counts']:\n",
    "            ax.plot(range(1, len(res['memory_counts'])+1), res['memory_counts'],\n",
    "                    label=name, linewidth=2,\n",
    "                    color=['#00d2ff', '#e94560', '#0f3460'][i % 3])\n",
    "            \n",
    "            # Mark flush points\n",
    "            for ft in res['flush_turns']:\n",
    "                if ft <= len(res['memory_counts']):\n",
    "                    ax.axvline(x=ft, alpha=0.15, linestyle='--',\n",
    "                              color=['#00d2ff', '#e94560', '#0f3460'][i % 3])\n",
    "    \n",
    "    # Mark probe positions\n",
    "    for pt in sorted(probes.keys()):\n",
    "        ax.axvline(x=pt, alpha=0.3, linestyle=':', color='yellow', linewidth=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Turn Number', fontsize=12)\n",
    "    ax.set_ylabel('Active Memories', fontsize=12)\n",
    "    ax.set_title('Memory Growth Over Conversation', fontsize=16, fontweight='bold', pad=15)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    rows = []\n",
    "    for name, res in all_results.items():\n",
    "        tl = np.array(res['turn_latencies']) if res['turn_latencies'] else np.array([0])\n",
    "        rl = np.array(res['retrieval_latencies']) if res['retrieval_latencies'] else np.array([0])\n",
    "        cu = np.array(res['context_utilizations']) if res['context_utilizations'] else np.array([0])\n",
    "        mc = np.array(res['memory_counts']) if res['memory_counts'] else np.array([0])\n",
    "        \n",
    "        probes_passed = sum(1 for p in res['probe_results'] if p['accuracy'] == 1.0)\n",
    "        total_probes = len(res['probe_results'])\n",
    "        \n",
    "        rows.append(f\"| {name} | {res['overall_accuracy']:.0%} ({probes_passed}/{total_probes}) | \"\n",
    "                    f\"{tl.mean():.0f}ms | {rl.mean():.1f}ms | \"\n",
    "                    f\"{cu.mean():.1%} | {mc[-1]} | {len(res['flush_turns'])} | {len(res['errors'])} |\")\n",
    "    \n",
    "    header = ('| Provider | Accuracy | Avg Latency | Avg Retrieval | '\n",
    "              'Ctx Usage | Final Mems | Flushes | Errors |\\n')\n",
    "    header += '|:---------|:--------:|:-----------:|:-------------:|'\n",
    "    header += ':--------:|:---------:|:-------:|:------:|\\n'\n",
    "    \n",
    "    display(Markdown('#### Comparison Table\\n\\n' + header + '\\n'.join(rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to JSON for later analysis\n",
    "if all_results:\n",
    "    export = {}\n",
    "    for name, res in all_results.items():\n",
    "        export[name] = {\n",
    "            'provider': res['provider'],\n",
    "            'model': res['model'],\n",
    "            'overall_accuracy': res['overall_accuracy'],\n",
    "            'turns_evaluated': res['turns_evaluated'],\n",
    "            'avg_turn_latency_ms': float(np.mean(res['turn_latencies'])) if res['turn_latencies'] else 0,\n",
    "            'avg_retrieval_latency_ms': float(np.mean(res['retrieval_latencies'])) if res['retrieval_latencies'] else 0,\n",
    "            'final_memory_count': res['memory_counts'][-1] if res['memory_counts'] else 0,\n",
    "            'total_flushes': len(res['flush_turns']),\n",
    "            'total_errors': len(res['errors']),\n",
    "            'probe_results': res['probe_results'],\n",
    "        }\n",
    "    \n",
    "    with open('eval/benchmark_results.json', 'w') as f:\n",
    "        json.dump(export, f, indent=2, default=str)\n",
    "    \n",
    "    print('\u2713 Results exported to eval/benchmark_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "### How Probes Work\n",
    "\n",
    "1. **Plants** are inserted at specific early turns (1, 2, 5, 12, 30, 45, 60, 80, 100, 120)\n",
    "   to teach the agent factual information about the user.\n",
    "2. **Probes** are questions at later turns that test whether the agent recalls that information.\n",
    "   - Early probes (8, 15, 25, 35, 48) test immediate retention\n",
    "   - Late probes (200, 350, 500, 600, 750, 850, 937, 1000) test long-term retention\n",
    "3. Accuracy is measured by keyword matching: does the response contain expected keywords?\n",
    "\n",
    "### Provider Notes\n",
    "\n",
    "- **Ollama/Mistral**: Fast locally, no API costs. Quality depends on model size.\n",
    "- **Gemini/Gemma 3 27B**: Strong quality, generous free tier (30 req/min). Runs take longer due to rate limiting.\n",
    "- **Groq/LLaMA 3.1**: Very fast inference but aggressive rate limits on free tier.\n",
    "\n",
    "### Running Your Own Benchmarks\n",
    "\n",
    "```bash\n",
    "# CLI evaluation\n",
    "uv run python eval/evaluate.py --provider gemini --model gemma-3-27b-it\n",
    "uv run python eval/evaluate.py --local --model mistral --turns 50\n",
    "uv run python eval/evaluate.py --provider gemini --quick  # Fast test\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}